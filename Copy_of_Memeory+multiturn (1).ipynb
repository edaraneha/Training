{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn1l2S-sittw"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers langchain chromadb pypdf faiss-cpu \\\n",
        "langchain_community scikit-learn numpy mistralai langchain-mistralai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6N7sqTmivg3"
      },
      "outputs": [],
      "source": [
        "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "from langchain_classic.chains import ConversationalRetrievalChain\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_5yMcECiwog"
      },
      "outputs": [],
      "source": [
        "os.environ[\"MISTRAL_API_KEY\"] = getpass(\"Enter Mistral API Key: \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMbA7WwdmXne"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlmtnI9yiyjr"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/FUNDAMENTALS OF MANAGEMENT.pdf\")   # upload your PDF to colab\n",
        "docs = loader.load()\n",
        "len(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr8AiRPdi0dn"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "len(chunks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyUnNNHYi2EI"
      },
      "outputs": [],
      "source": [
        "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
        "\n",
        "vector_db = FAISS.from_documents(chunks, embedding=embeddings)\n",
        "retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UG7DvX4i3_v"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6Vc5L4Ni5jM"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "You are a Memory-Augmented RAG assistant.\n",
        "\n",
        "Conversation History:\n",
        "{chat_history}\n",
        "\n",
        "Relevant Retrieved Context:\n",
        "{context}\n",
        "\n",
        "User Query:\n",
        "{question}\n",
        "\n",
        "Provide a context-aware answer. ALWAYS use history if relevant.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"context\", \"question\"],\n",
        "    template=template\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqGSfSisi7BF"
      },
      "outputs": [],
      "source": [
        "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
        "\n",
        "rag_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    combine_docs_chain_kwargs={\"prompt\": prompt}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtYOic8Ui9fS"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    q = input(\"You: \")\n",
        "    if q.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "    resp = rag_chain.invoke({\"question\": q})\n",
        "    print(\"Bot:\", resp[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-doadh0czyAo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}